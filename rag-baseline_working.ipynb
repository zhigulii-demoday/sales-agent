{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### Classic RAG"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T15:25:23.443426Z","iopub.status.busy":"2024-09-10T15:25:23.441683Z","iopub.status.idle":"2024-09-10T15:25:35.043158Z","shell.execute_reply":"2024-09-10T15:25:35.041674Z","shell.execute_reply.started":"2024-09-10T15:25:23.443328Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["Requirement already satisfied: psycopg2-binary in ./venv_llm/lib/python3.12/site-packages (2.9.9)\n","Requirement already satisfied: SQLAlchemy in ./venv_llm/lib/python3.12/site-packages (2.0.34)\n","Requirement already satisfied: sqlparse in ./venv_llm/lib/python3.12/site-packages (0.5.1)\n","Requirement already satisfied: typing-extensions>=4.6.0 in ./venv_llm/lib/python3.12/site-packages (from SQLAlchemy) (4.12.2)\n","Requirement already satisfied: greenlet!=0.4.17 in ./venv_llm/lib/python3.12/site-packages (from SQLAlchemy) (3.0.3)\n"]}],"source":["!pip install psycopg2-binary SQLAlchemy sqlparse"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-10T10:58:15.755725Z","iopub.status.busy":"2024-09-10T10:58:15.755318Z","iopub.status.idle":"2024-09-10T10:58:20.884480Z","shell.execute_reply":"2024-09-10T10:58:20.883086Z","shell.execute_reply.started":"2024-09-10T10:58:15.755686Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["/home/zhigul/llm/venv_llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n"]}],"source":["import pandas as pd\n","import torch\n","from sklearn.metrics.pairwise import cosine_similarity\n","from transformers import AutoTokenizer, AutoModel\n","\n","from llama_cpp import Llama\n","from huggingface_hub import hf_hub_download\n","\n","import psycopg2 as pg\n","import sqlparse\n","engine = pg.connect(\"dbname='sales' user='zhigul' host='89.169.163.130' port='5432' password='asd'\")\n"]},{"cell_type":"code","execution_count":3,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T10:58:01.742873Z","iopub.status.busy":"2024-09-10T10:58:01.742427Z","iopub.status.idle":"2024-09-10T10:58:01.768038Z","shell.execute_reply":"2024-09-10T10:58:01.766962Z","shell.execute_reply.started":"2024-09-10T10:58:01.742824Z"},"trusted":true},"outputs":[],"source":["class BertEmbedder:\n","    def __init__(self, model_name, device='cpu'):\n","        \"\"\"\n","        Инициализация BERT модели и токенайзера.\n","        \n","        :param model_name: Название предобученной модели).\n","        :param device: Устройство для вычислений ('cuda' для GPU или 'cpu' для CPU). Если None, автоматически выбирается доступное устройство.\n","        \"\"\"\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n","        print(device)\n","        \n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModel.from_pretrained(model_name)\n","        if self.device == 'cuda':\n","            self.model.cuda()\n","\n","    def get_embedding(self, text):\n","        \"\"\"\n","        Преобразует текст в эмбеддинг с использованием BERT.\n","        \n","        :param text: Входной текст для преобразования.\n","        :return: Эмбеддинг текста в виде numpy массива.\n","        \"\"\"\n","        tokenized_text = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n","        \n","        with torch.no_grad():\n","            model_output = self.model(**{k: v.to(self.device) for k, v in tokenized_text.items()})\n","        \n","        embeddings = model_output.last_hidden_state[:, 0, :]\n","        embeddings = torch.nn.functional.normalize(embeddings)\n","        return embeddings[0].cpu().numpy()\n","    \n","    def retrieve_reviews(self, query, reviews_df, top_n=3):\n","        review_texts = reviews_df['Review Text'].fillna('').tolist()\n","        \n","        reviews_embeddings = self.get_embedding(review_texts)\n","        query_embedding = self.get_embedding(query)\n","        similarities = cosine_similarity(query_embedding, reviews_embeddings).flatten()\n","        top_indices = similarities.argsort()[-top_n:][::-1]\n","\n","        return reviews_df.iloc[top_indices]\n","    "]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[],"source":["class LLMModel:\n","    def __init__(self, model_id, filename_gguf, device='cpu'):\n","        \"\"\"\n","        Инициализация LLM модели.\n","        \n","        :param model_name: Название предобученной модели.\n","        :param device: Устройство для вычислений ('cuda' для GPU или 'cpu' для CPU). Если None, автоматически выбирается доступное устройство.\n","        \"\"\"\n","        # self.tokenizer = AutoTokenizer.from_pretrained('meta-llama/Meta-Llama-3.1-8B-Instruct')\n","        # model_path = hf_hub_download(repo_id=model_id, filename=filename_gguf)\n","        # self.model = Llama(\n","        #     model_path=model_path,\n","        #     n_gpu_layers=0,\n","        #     n_threads=16,\n","        #     n_ctx=4096\n","        # )\n","        self.generation_config = {\n","            \"max_tokens\": 20000,\n","            \"stop\": [\"<|endoftext|>\"],\n","            \"echo\": False,\n","            \"n_ctx\": 4096\n","        } \n","        self.sql_llm = Llama.from_pretrained(\n","            repo_id=\"TheBloke/CodeLlama-7B-Instruct-GGUF\",\n","            filename=\"codellama-7b-instruct.Q4_0.gguf\",\n","            **self.generation_config\n","            )\n","\n","        self.chat_llm = Llama.from_pretrained(\n","            repo_id=\"bartowski/llama3-turbcat-instruct-8b-GGUF\",\n","            filename=\"llama3-turbcat-instruct-8b-Q4_K_S.gguf\",\n","            **self.generation_config\n","            )\n","\n","\n","    # def generate_first_message(self):\n","    #     context_1 = \"\"\" Napoleon IT Отзывы - система для интеллектуального анализа и работы с пользовательским фидбэком. В том числе и на англоязычном рынке\n","    #     Как мы уже это сделали в Cotton Club и Lapochka.\n","    #     Sales & Operations\n","    #     -  Отслеживание качества обслуживания: Контроль качества обслуживания клиентов на всех этапах их взаимодействия, от оформления заказа до доставки\n","    #     -  Оптимизация ассортимента: Выявление наиболее популярных и востребованных позиций, а также тех, которые требуют улучшения\n","    #     Brand Management & Consumer Market Insights (CMI) \n","    #     -  Отслеживание реакции на кампании: Мониторинг изменения восприятия до, во время и после рекламных кампаний\n","    #     RnD\n","    #     -  Тестирование нововведений: Анализ отзывов о новых продуктах и услугах, чтобы понять их восприятие и эффективность\n","    #     -  Анализ трендов: Отслеживание трендов в индустрии, чтобы своевременно внедрять актуальные изменения и оставаться конкурентоспособными\n","    #     \"\"\"\n","    #     system_prompt = f\"\"\"Ты профессиональный агент-продавец, опыт которого в продажах больше 10 лет.\n","    #         Ты инициализируешь диалог с клиентом и твоя цель - заинтересовать его продуктом, который ты предлагаешь.\n","    #         Ты должен быть убедительным и дружелюбным. Обращайся на 'Вы', но не используй в письме вступление в виде 'Уважаемый ...' и заключение по типу 'С уважением..'\n","    #         Твои ответы должны быть только на Русском.\n","\n","    #         Продукт, который ты предлагаешь описан тут:\n","    #         {context_1}\n","\n","    #         Покажи уверенные навыки продавца и заинтересуй клиента:\n","    #         \"\"\"\n","\n","    #     user_prompt = f\"Я - потенциальный клиент. Инициируй со мной диалог: Напиши мне письмо, где рассказываешь о продукте Napoleon Отзывы.\"\n","\n","    #     message = [\n","    #         {'role': 'system', 'content': system_prompt},\n","    #         {'role': 'user', 'content': user_prompt}\n","    #     ]\n","        \n","    #     # prompt = self.tokenizer.apply_chat_template(message, tokenize=False).rstrip('<|endoftext|>').strip() + '\\n<|assistant|>' # tokenize=False => в prompt будет храниться строка со спец.токенами\n","    #     # llm = self.model(prompt, **self.generation_config)\n","    #     result = self.llm.create_chat_completion(messages = message)\n","\n","    #     print(f\"Ответ модели: {result['choices'][0]['message']['content']}\")\n","\n","    def classify_rag_or_text2sql(self, client_query):\n","        system_prompt = f\"\"\"Ты должен будешь определить, можно-ли перевести отправленое тебе предложение в SQL. \n","                            Ты должен отвечать на вопросы только 'Да' и 'Нет'.\n","                        \"\"\"\n","        user_prompt = client_query\n","\n","        message = [\n","            {'role': 'system', 'content': system_prompt},\n","            {'role': 'user', 'content': user_prompt}\n","        ]\n","        result = self.chat_llm.create_chat_completion(messages=message)\n","        print(f\"Ответ модели: {result['choices'][0]['message']['content']}\")\n","        return result['choices'][0]['message']['content']\n","\n","    def generate_text2sql_response(self, client_query):\n","        schema = \"\"\"CREATE TABLE reviews (\n","        id INTEGER DEFAULT nextval('reviews_id_seq'::regclass) NOT NULL, \n","        company_id INTEGER NOT NULL, \n","        product_cat VARCHAR, \n","        product_name VARCHAR, \n","        review_dt TIMESTAMP, \n","        review_text VARCHAR, \n","        topic VARCHAR, \n","        sentiment VARCHAR, \n","        marketplace VARCHAR, \n","        embedding VECTOR(20), \n","        created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, \n","        updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP, \n","        deleted_at TIMESTAMP, \n","        CONSTRAINT reviews_pkey PRIMARY KEY (id), \n","        CONSTRAINT reviews_company_id_fkey FOREIGN KEY(company_id) REFERENCES companies (id)\n","            )      \n","        \"\"\"\n","        sql_example = \"SELECT * FROM reviews WHERE topic == 'Вкус\"\n","        user_prompt = client_query\n","        system_prompt = f\"\"\"\n","                Generate a Postgresql SQL query to answer the following question:\n","                `{user_prompt}\n","                Please wrap your code answer using ```sql:\n","                ### Database Schema\n","                This query will run on a database whose schema is represented in this string:\n","                Don't use joins for this schema and if all columns are required give the (*) notation.\n","                `{schema}`\n","                An example of the SQL would be `{sql_example}`\n","                ### SQL\n","                Given the database schema, here is the SQL query that answers `{user_prompt}`:\n","                ```sql\n","                \"\"\"\n","\n","        message = [\n","            {'role': 'system', 'content': system_prompt},\n","            {'role': 'user', 'content': user_prompt}\n","        ]\n","        result = self.sql_llm.create_chat_completion(messages=message)\n","        print(f\"Ответ модели: {sqlparse.format(result['choices'][0]['message']['content'].split(\"```\")[-2], reindent=True).replace('#','').replace(\"sql \",\"\")}\")\n","\n","    def generate_rag_response(self, client_query, relevant_reviews):\n","        review_texts = '\\n'.join(relevant_reviews['Review Text'].tolist())\n","        system_prompt = f\"\"\"Ты профессиональный агент-продавец, опыт которого в продажах больше 10 лет.\n","            Ты ведешь диалог с клиентом, которому предлагаешь сотрудничество по продукту \"Napoleon IT Отзывы\".\n","            Ты должен обрабатывать любые вопросы потенциального клиента, предоставляя только релевантную и достовернную информацию, используя контекст.\n","            Не груби и будь дружелюбным собесебником.\n","            Отвечай только на Русском.\n","\n","            Обработай запрос клиента и выдай точную информацию:\n","            Вот отзывы о товарах клиента: {review_texts}. Ответь на вопрос клиента на опираясь на них.\n","            \"\"\"\n","        user_prompt = f\"Клиент спросил: '{client_query}'\"\n","\n","        message = [\n","            {'role': 'system', 'content': system_prompt},\n","            {'role': 'user', 'content': user_prompt}\n","        ]\n","        \n","        result = self.chat_llm.create_chat_completion(messages=message)\n","        \n","        print(f\"Ответ модели: {result['choices'][0]['text']}\")"]},{"cell_type":"code","execution_count":5,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T10:58:01.928800Z","iopub.status.busy":"2024-09-10T10:58:01.928394Z","iopub.status.idle":"2024-09-10T10:58:01.936810Z","shell.execute_reply":"2024-09-10T10:58:01.935447Z","shell.execute_reply.started":"2024-09-10T10:58:01.928763Z"},"trusted":true},"outputs":[],"source":["class SalesBotHandler:\n","    def __init__(self,\n","                 bert_model_name='cointegrated/rubert-tiny', \n","                 llm_model_name='TheBloke/CodeLlama-7B-Instruct-GGUF',\n","                 llm_model_path='codellama-7b-instruct.Q4_0.gguf',\n","                 reviews_path = \"/home/zhigul/llm/cleaned_coffee.csv\"):\n","        self.bert_embedder = BertEmbedder(model_name=bert_model_name)\n","        self.llm_model = LLMModel(model_id=llm_model_name, filename_gguf=llm_model_path)\n","        self.reviews = self.load_reviews(reviews_path)\n","    \n","    def load_reviews(self, reviews_path):\n","        return pd.read_csv(reviews_path)\n","    \n","    def process_customer_input(self, customer_input):\n","        \"\"\"\n","        Метод для ведения диалога с клиентом с использованием LLM Model.\n","        \"\"\"\n","        # relevant_reviews = self.bert_embedder.retrieve_reviews(customer_input, self.reviews)\n","        is_text2sql = self.llm_model.classify_rag_or_text2sql(customer_input)\n","        if 'Да' in is_text2sql:\n","            print('ЭТО TEXT2SQL')\n","            return self.llm_model.generate_text2sql_response(customer_input)\n","        else:\n","            print('ЭТО ПРОСТО RAG')\n","            # return self.llm_model.generate_rag_response(customer_input, relevant_reviews)\n","\n","    def generate_first_message(self):\n","        \"\"\"\n","        Метод для генерации первого письма с использованием LLM Model.\n","        \"\"\"\n","        return self.llm_model.generate_first_message()"]},{"cell_type":"code","execution_count":6,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T10:58:02.363808Z","iopub.status.busy":"2024-09-10T10:58:02.363390Z","iopub.status.idle":"2024-09-10T10:58:02.675587Z","shell.execute_reply":"2024-09-10T10:58:02.674046Z","shell.execute_reply.started":"2024-09-10T10:58:02.363768Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cpu\n"]},{"name":"stderr","output_type":"stream","text":["/home/zhigul/llm/venv_llm/lib/python3.12/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n","llama_model_loader: loaded meta data with 20 key-value pairs and 291 tensors from /home/zhigul/.cache/huggingface/hub/models--TheBloke--CodeLlama-7B-Instruct-GGUF/snapshots/2f064ee0c6ae3f025ec4e392c6ba5dd049c77969/./codellama-7b-instruct.Q4_0.gguf (version GGUF V2)\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = codellama_codellama-7b-instruct-hf\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 16384\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 11008\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 32\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 1000000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 2\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32016]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32016]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32016]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  17:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  18:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  19:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_0:  225 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: special tokens cache size = 3\n","llm_load_vocab: token to piece cache size = 0.1686 MB\n","llm_load_print_meta: format           = GGUF V2\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = SPM\n","llm_load_print_meta: n_vocab          = 32016\n","llm_load_print_meta: n_merges         = 0\n","llm_load_print_meta: vocab_only       = 0\n","llm_load_print_meta: n_ctx_train      = 16384\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 32\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_swa            = 0\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 1\n","llm_load_print_meta: n_embd_k_gqa     = 4096\n","llm_load_print_meta: n_embd_v_gqa     = 4096\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 11008\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 1000000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_ctx_orig_yarn  = 16384\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: ssm_dt_b_c_rms   = 0\n","llm_load_print_meta: model type       = 7B\n","llm_load_print_meta: model ftype      = Q4_0\n","llm_load_print_meta: model params     = 6.74 B\n","llm_load_print_meta: model size       = 3.56 GiB (4.54 BPW) \n","llm_load_print_meta: general.name     = codellama_codellama-7b-instruct-hf\n","llm_load_print_meta: BOS token        = 1 '<s>'\n","llm_load_print_meta: EOS token        = 2 '</s>'\n","llm_load_print_meta: UNK token        = 0 '<unk>'\n","llm_load_print_meta: LF token         = 13 '<0x0A>'\n","llm_load_print_meta: PRE token        = 32007 '▁<PRE>'\n","llm_load_print_meta: SUF token        = 32008 '▁<SUF>'\n","llm_load_print_meta: MID token        = 32009 '▁<MID>'\n","llm_load_print_meta: EOT token        = 32010 '▁<EOT>'\n","llm_load_print_meta: max token length = 48\n","llm_load_tensors: ggml ctx size =    0.14 MiB\n","llm_load_tensors:        CPU buffer size =  3647.95 MiB\n","..................................................................................................\n","llama_new_context_with_model: n_ctx      = 4096\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 1000000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =  2048.00 MiB\n","llama_new_context_with_model: KV self size  = 2048.00 MiB, K (f16): 1024.00 MiB, V (f16): 1024.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '1000000.000000', 'llama.context_length': '16384', 'general.name': 'codellama_codellama-7b-instruct-hf', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '11008', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '128', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '32', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '2'}\n","Using fallback chat format: llama-2\n","llama_model_loader: loaded meta data with 27 key-value pairs and 291 tensors from /home/zhigul/.cache/huggingface/hub/models--bartowski--llama3-turbcat-instruct-8b-GGUF/snapshots/719e6053546de42fb09a24eb3e4c7265576c7e33/./llama3-turbcat-instruct-8b-Q4_K_S.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = llama3-turbcat-instruct-8b\n","llama_model_loader: - kv   2:                          llama.block_count u32              = 32\n","llama_model_loader: - kv   3:                       llama.context_length u32              = 8192\n","llama_model_loader: - kv   4:                     llama.embedding_length u32              = 4096\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 14336\n","llama_model_loader: - kv   6:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   7:              llama.attention.head_count_kv u32              = 8\n","llama_model_loader: - kv   8:                       llama.rope.freq_base f32              = 500000.000000\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                          general.file_type u32              = 14\n","llama_model_loader: - kv  11:                           llama.vocab_size u32              = 128256\n","llama_model_loader: - kv  12:                 llama.rope.dimension_count u32              = 128\n","llama_model_loader: - kv  13:                       tokenizer.ggml.model str              = gpt2\n","llama_model_loader: - kv  14:                         tokenizer.ggml.pre str              = llama-bpe\n","llama_model_loader: - kv  15:                      tokenizer.ggml.tokens arr[str,128256]  = [\"!\", \"\\\"\", \"#\", \"$\", \"%\", \"&\", \"'\", ...\n","llama_model_loader: - kv  16:                  tokenizer.ggml.token_type arr[i32,128256]  = [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, ...\n","llama_model_loader: - kv  17:                      tokenizer.ggml.merges arr[str,280147]  = [\"Ġ Ġ\", \"Ġ ĠĠĠ\", \"ĠĠ ĠĠ\", \"...\n","llama_model_loader: - kv  18:                tokenizer.ggml.bos_token_id u32              = 128000\n","llama_model_loader: - kv  19:                tokenizer.ggml.eos_token_id u32              = 128009\n","llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 128001\n","llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% if not add_generation_prompt is de...\n","llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n","llama_model_loader: - kv  23:                      quantize.imatrix.file str              = /models/llama3-turbcat-instruct-8b-GG...\n","llama_model_loader: - kv  24:                   quantize.imatrix.dataset str              = /training_data/calibration_datav3.txt\n","llama_model_loader: - kv  25:             quantize.imatrix.entries_count i32              = 224\n","llama_model_loader: - kv  26:              quantize.imatrix.chunks_count i32              = 125\n","llama_model_loader: - type  f32:   65 tensors\n","llama_model_loader: - type q4_K:  217 tensors\n","llama_model_loader: - type q5_K:    8 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","llm_load_vocab: special tokens cache size = 256\n","llm_load_vocab: token to piece cache size = 0.8000 MB\n","llm_load_print_meta: format           = GGUF V3 (latest)\n","llm_load_print_meta: arch             = llama\n","llm_load_print_meta: vocab type       = BPE\n","llm_load_print_meta: n_vocab          = 128256\n","llm_load_print_meta: n_merges         = 280147\n","llm_load_print_meta: vocab_only       = 0\n","llm_load_print_meta: n_ctx_train      = 8192\n","llm_load_print_meta: n_embd           = 4096\n","llm_load_print_meta: n_layer          = 32\n","llm_load_print_meta: n_head           = 32\n","llm_load_print_meta: n_head_kv        = 8\n","llm_load_print_meta: n_rot            = 128\n","llm_load_print_meta: n_swa            = 0\n","llm_load_print_meta: n_embd_head_k    = 128\n","llm_load_print_meta: n_embd_head_v    = 128\n","llm_load_print_meta: n_gqa            = 4\n","llm_load_print_meta: n_embd_k_gqa     = 1024\n","llm_load_print_meta: n_embd_v_gqa     = 1024\n","llm_load_print_meta: f_norm_eps       = 0.0e+00\n","llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n","llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n","llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n","llm_load_print_meta: f_logit_scale    = 0.0e+00\n","llm_load_print_meta: n_ff             = 14336\n","llm_load_print_meta: n_expert         = 0\n","llm_load_print_meta: n_expert_used    = 0\n","llm_load_print_meta: causal attn      = 1\n","llm_load_print_meta: pooling type     = 0\n","llm_load_print_meta: rope type        = 0\n","llm_load_print_meta: rope scaling     = linear\n","llm_load_print_meta: freq_base_train  = 500000.0\n","llm_load_print_meta: freq_scale_train = 1\n","llm_load_print_meta: n_ctx_orig_yarn  = 8192\n","llm_load_print_meta: rope_finetuned   = unknown\n","llm_load_print_meta: ssm_d_conv       = 0\n","llm_load_print_meta: ssm_d_inner      = 0\n","llm_load_print_meta: ssm_d_state      = 0\n","llm_load_print_meta: ssm_dt_rank      = 0\n","llm_load_print_meta: ssm_dt_b_c_rms   = 0\n","llm_load_print_meta: model type       = 8B\n","llm_load_print_meta: model ftype      = Q4_K - Small\n","llm_load_print_meta: model params     = 8.03 B\n","llm_load_print_meta: model size       = 4.36 GiB (4.67 BPW) \n","llm_load_print_meta: general.name     = llama3-turbcat-instruct-8b\n","llm_load_print_meta: BOS token        = 128000 '<|begin_of_text|>'\n","llm_load_print_meta: EOS token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: PAD token        = 128001 '<|end_of_text|>'\n","llm_load_print_meta: LF token         = 128 'Ä'\n","llm_load_print_meta: EOT token        = 128009 '<|eot_id|>'\n","llm_load_print_meta: max token length = 256\n","llm_load_tensors: ggml ctx size =    0.14 MiB\n","llm_load_tensors:        CPU buffer size =  4467.80 MiB\n",".......................................................................................\n","llama_new_context_with_model: n_ctx      = 4096\n","llama_new_context_with_model: n_batch    = 512\n","llama_new_context_with_model: n_ubatch   = 512\n","llama_new_context_with_model: flash_attn = 0\n","llama_new_context_with_model: freq_base  = 500000.0\n","llama_new_context_with_model: freq_scale = 1\n","llama_kv_cache_init:        CPU KV buffer size =   512.00 MiB\n","llama_new_context_with_model: KV self size  =  512.00 MiB, K (f16):  256.00 MiB, V (f16):  256.00 MiB\n","llama_new_context_with_model:        CPU  output buffer size =     0.49 MiB\n","llama_new_context_with_model:        CPU compute buffer size =   296.01 MiB\n","llama_new_context_with_model: graph nodes  = 1030\n","llama_new_context_with_model: graph splits = 1\n","AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n","Model metadata: {'quantize.imatrix.entries_count': '224', 'quantize.imatrix.dataset': '/training_data/calibration_datav3.txt', 'quantize.imatrix.chunks_count': '125', 'quantize.imatrix.file': '/models/llama3-turbcat-instruct-8b-GGUF/llama3-turbcat-instruct-8b.imatrix', 'tokenizer.chat_template': \"{% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\\n\\n'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\\n\\n' }}{% endif %}\", 'tokenizer.ggml.padding_token_id': '128001', 'tokenizer.ggml.eos_token_id': '128009', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'gpt2', 'general.architecture': 'llama', 'llama.rope.freq_base': '500000.000000', 'tokenizer.ggml.pre': 'llama-bpe', 'llama.context_length': '8192', 'general.name': 'llama3-turbcat-instruct-8b', 'llama.embedding_length': '4096', 'llama.feed_forward_length': '14336', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'tokenizer.ggml.bos_token_id': '128000', 'llama.attention.head_count': '32', 'llama.block_count': '32', 'llama.attention.head_count_kv': '8', 'general.file_type': '14', 'llama.vocab_size': '128256', 'llama.rope.dimension_count': '128'}\n","Available chat formats from metadata: chat_template.default\n","Using gguf chat template: {% if not add_generation_prompt is defined %}{% set add_generation_prompt = false %}{% endif %}{% set loop_messages = messages %}{% for message in loop_messages %}{% set content = '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n","\n","'+ message['content'] | trim + '<|eot_id|>' %}{% if loop.index0 == 0 %}{% set content = bos_token + content %}{% endif %}{{ content }}{% endfor %}{% if add_generation_prompt %}{{ '<|start_header_id|>assistant<|end_header_id|>\n","\n","' }}{% endif %}\n","Using chat eos_token: <|eot_id|>\n","Using chat bos_token: <|begin_of_text|>\n"]}],"source":["handler = SalesBotHandler()"]},{"cell_type":"code","execution_count":7,"metadata":{"execution":{"iopub.execute_input":"2024-09-10T10:58:02.748794Z","iopub.status.busy":"2024-09-10T10:58:02.747988Z","iopub.status.idle":"2024-09-10T10:58:02.771728Z","shell.execute_reply":"2024-09-10T10:58:02.770304Z","shell.execute_reply.started":"2024-09-10T10:58:02.748752Z"},"trusted":true},"outputs":[{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =    5661.95 ms\n","llama_print_timings:      sample time =       0.40 ms /     3 runs   (    0.13 ms per token,  7481.30 tokens per second)\n","llama_print_timings: prompt eval time =    5661.77 ms /    69 tokens (   82.05 ms per token,    12.19 tokens per second)\n","llama_print_timings:        eval time =     300.88 ms /     2 runs   (  150.44 ms per token,     6.65 tokens per second)\n","llama_print_timings:       total time =    5967.92 ms /    71 tokens\n"]},{"name":"stdout","output_type":"stream","text":["Ответ модели: Да\n","ЭТО TEXT2SQL\n"]},{"name":"stderr","output_type":"stream","text":["\n","llama_print_timings:        load time =   54194.34 ms\n","llama_print_timings:      sample time =       5.14 ms /   124 runs   (    0.04 ms per token, 24110.44 tokens per second)\n","llama_print_timings: prompt eval time =   54193.86 ms /   405 tokens (  133.81 ms per token,     7.47 tokens per second)\n","llama_print_timings:        eval time =   27376.00 ms /   123 runs   (  222.57 ms per token,     4.49 tokens per second)\n","llama_print_timings:       total time =   81659.31 ms /   528 tokens\n"]},{"name":"stdout","output_type":"stream","text":["Ответ модели: sql\n","SELECT COUNT(*)\n","FROM reviews\n","WHERE sentiment = 'positive'\n","  AND company_id = <your_company_id>;\n"]}],"source":["# handler.generate_first_message()\n","\n","client_query = \"Сколько положительных отзывов о моих товарах?\"\n","relevant_reviews = handler.process_customer_input(client_query)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5655567,"sourceId":9334355,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
