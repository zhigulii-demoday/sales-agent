{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Импорта библиотек"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhigul/llm/venv_llm/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "from typing import Callable\n",
    "\n",
    "import torch\n",
    "from llama_cpp import Llama\n",
    "from huggingface_hub import hf_hub_download\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n",
    "from langchain_experimental.utilities.python import PythonREPL\n",
    "\n",
    "os.environ['HF_HUB_OFFLINE'] = '0'\n",
    "os.environ['HUGGINGFACE_HUB_CACHE'] = './hugging_face_hub'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Системные инструкции + контекст"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "### general context about \"Napoleon IT Review\"\n",
    "CONTEXT_NAPOLEON_IT = \"\"\"Что такое \"Napoleon IT Отзывы\" и зачем нужен это продукт:\n",
    "Sales & Operations:\n",
    "-  Отслеживание качества обслуживания: Контроль качества обслуживания клиентов на всех этапах их взаимодействия, от оформления заказа до доставки\n",
    "-  Анализ удовлетворенности клиентов: Отслеживание метрик удовлетворенности и выявление слабых мест для их устранения\n",
    "-  Мониторинг конкурентов: Анализ отзывов и оценок конкурентов для понимания их сильных и слабых сторон\n",
    "-  Оптимизация ассортимента: Выявление наиболее популярных и востребованных позиций, а также тех, которые требуют улучшения\n",
    "Brand Management & Consumer Market Insights (CMI):\n",
    "-  Мониторинг восприятия бренда: Изучение отзывов для понимания, как клиенты воспринимают ваш бренд и его уникальные предложения\n",
    "-  Оптимизация маркетинговых сообщений: Выбор наиболее эффективных сообщений для рекламных кампаний на основе анализа фидбэка\n",
    "-  Отслеживание реакции на кампании: Мониторинг изменения восприятия до, во время и после рекламных кампаний\n",
    "RnD:\n",
    "-  Улучшение качества продуктов: Выявление, какие аспекты вашей продукции клиенты оценивают положительно, а какие требуют улучшения\n",
    "-  Тестирование нововведений: Анализ отзывов о новых продуктах и услугах, чтобы понять их восприятие и эффективность\n",
    "-  Анализ трендов: Отслеживание трендов в индустрии, чтобы своевременно внедрять актуальные изменения и оставаться конкурентоспособными\n",
    "Эта система поможет вам глубже понять потребности и ожидания ваших клиентов, улучшить качество обслуживания и ассортимент продукции, что в конечном итоге повысит лояльность покупателей и увеличит продажи.\"\"\"\n",
    "\n",
    "SALES_SCRIPT = \"\"\"1. Поприветствуй клиента и представься сам.\n",
    "2. Расскажи о выгодном предложении, которое ты ему принес.\n",
    "3. Выдели ключевые преимущества твоего предложения и то, как это решение может помочь бизнесу клиента (опиши очень кратко).\n",
    "4. Подтолкни клиента к ответу, задав вопрос о дальнейшей коммуникации и поинтересуйся, как клиент оценивает предоставленное предложение.\n",
    "5. Не забудь попращаться на дружественной ноте и зафиксировать, что ждешь ответ от клиента.\"\"\"\n",
    "\n",
    "\n",
    "### general system prompts\n",
    "SYS_PROMPT_INIT_DIALOGUE = lambda context_1, sales_script: f\"\"\"Ты - AI-ассистент в роли профессионального агента-продавца, опыт которого в продажах больше 10 лет.\n",
    "Ты инициализируешь диалог с клиентом через переписку и твоя цель - заинтересовать его продуктом, который ты предлагаешь.\n",
    "Ты должен быть уважительным.\n",
    "Не зацикливайся и выдавай убедительные предложения, которые могут заинтересовать клиента.\n",
    "Твои ответы должны быть только на Русском.\n",
    "\n",
    "Продукт, который ты предлагаешь описан тут:\n",
    "{context_1}\n",
    "\n",
    "В своем ответе используй следующие подсказки:\n",
    "{sales_script}\n",
    "\n",
    "Строго соблюдай каждую инструкцию и структуру ответа.\n",
    "Покажи по-настоящему свои навыки продавца и заинтересуй клиента:\n",
    "\"\"\"\n",
    "\n",
    "USER_INIT_PROMPT = \"Следуй точно инструкциями и начни диалог с клиентом:\"\n",
    "\n",
    "\n",
    "SYS_PROMPT_CONTINUE_DIALOGUE = lambda context_2: f\"\"\"Ты - профессиональный агент-продавец, опыт которого в продажах больше 10 лет.\n",
    "Ты ведешь диалог через переписку с другим бизнесом, которому предлагаешь сотрудничество по продукту \"Napoleon IT Отзывы\".\n",
    "\"Napoleon IT Отзывы\" - платформа, которая предлагает бизнесам следить за актуальной информацией о своих продуктах и услугах.\n",
    "Таким образом, другие бизнесы могут стать нашими клиентами и получать только релевантную и достовернную информацию по своим продуктам.\n",
    "Ты должен обрабатывать любые вопросы клиента по его продукту, используя контекст.\n",
    "Отвечай только на Русском.\n",
    "\n",
    "Релевантные данные о продукте клиента приведены ниже:\n",
    "{context_2}\n",
    "\n",
    "Обработай запрос клиента и выдай точную информацию:\n",
    "\"\"\"\n",
    "\n",
    "SYS_PROMPT_FOR_INTENTION = \"\"\"Ты модель-классификатор, которая умеет четко определять намерение пользователя по его запросу.\n",
    "На вход тебе поступает пользовательский запрос и твоя задача - бинарно определить, относится ли данный запрос к продукту \"Napoleon IT Отзывы\".\n",
    "Строго выдай только бинарную метку \"да\" либо \"нет\" - без дополнительных комментариев и текста:\n",
    "\"\"\"\n",
    "\n",
    "SYS_PROMPT_TEXT_2_PANDAS = lambda path_to_data, columns, example: f\"\"\"Ты профессиональный программист и ML-разработчик.\n",
    "Ты пишешь качественный Python-код, используя библиотеку pandas, datetime, numpy и другие в зависимости от задачи.\n",
    "Также тебе предоставлен датасет в формате .csv по пути {path_to_data}.\n",
    "Датасет имеет следующие столбцы: {columns}.\n",
    "Пример строки представлен тут: {example}.\n",
    "\n",
    "Напиши код на Python, который сможет достать наиболее релевантные данные из датасета по запросу пользователя.\n",
    "Строго соблюдай следующий формат ответа:\n",
    "\n",
    "```\n",
    "[Программный код]\n",
    "```\n",
    "Никаких пояснений и комментариев до и после кода писать не нужно, СТРОГО следуй этому правилу:\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Класс SalesAgent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SalesAgent:\n",
    "\n",
    "    def __init__(self, \n",
    "                 model_id, \n",
    "                 repo_gguf_id, \n",
    "                 filename_gguf, \n",
    "                 load_gguf_config: dict) -> None:\n",
    "\n",
    "        self.model_id = model_id\n",
    "        self.repo_gguf_id = repo_gguf_id\n",
    "        self.filename_gguf = filename_gguf\n",
    "        self.load_gguf_config = load_gguf_config\n",
    "\n",
    "        self.model = self.load_gguf_model()\n",
    "\n",
    "\n",
    "    def load_gguf_model(self) -> Llama:\n",
    "\n",
    "        model_path = hf_hub_download(repo_id=self.repo_gguf_id, filename=self.filename_gguf)\n",
    "        \n",
    "        model = Llama(\n",
    "            model_path,\n",
    "            **self.load_gguf_config\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "\n",
    "    def inference_agent(self, messages: list[dict]) -> str:\n",
    "        response = self.model.create_chat_completion(messages)\n",
    "\n",
    "        return response['choices'][0]['content']\n",
    "    \n",
    "\n",
    "    def formating_chat_template(self, \n",
    "                                system: Callable,\n",
    "                                context: list[str],\n",
    "                                user: list[str],\n",
    "                                assistan: list[str]) -> list[dict]:\n",
    "        messages = [\n",
    "                {'role': 'system', 'content': system(*context)}\n",
    "        ]\n",
    "\n",
    "        for assistant_replica, user_replica in zip(assistan, user):\n",
    "            \n",
    "            messages.append(assistant_replica)\n",
    "            messages.append(user_replica)\n",
    "\n",
    "        return messages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "llama_model_loader: loaded meta data with 36 key-value pairs and 197 tensors from /home/zhigul/.cache/huggingface/hub/models--QuantFactory--Phi-3.5-mini-instruct-GGUF/snapshots/cb88a6c948766fc240bc811bf4d7f2f4ba94afaf/Phi-3.5-mini-instruct.Q8_0.gguf (version GGUF V3 (latest))\n",
      "llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n",
      "llama_model_loader: - kv   0:                       general.architecture str              = phi3\n",
      "llama_model_loader: - kv   1:                               general.type str              = model\n",
      "llama_model_loader: - kv   2:                               general.name str              = Phi 3.5 Mini Instruct\n",
      "llama_model_loader: - kv   3:                           general.finetune str              = instruct\n",
      "llama_model_loader: - kv   4:                           general.basename str              = Phi-3.5\n",
      "llama_model_loader: - kv   5:                         general.size_label str              = mini\n",
      "llama_model_loader: - kv   6:                            general.license str              = mit\n",
      "llama_model_loader: - kv   7:                       general.license.link str              = https://huggingface.co/microsoft/Phi-...\n",
      "llama_model_loader: - kv   8:                               general.tags arr[str,3]       = [\"nlp\", \"code\", \"text-generation\"]\n",
      "llama_model_loader: - kv   9:                          general.languages arr[str,1]       = [\"multilingual\"]\n",
      "llama_model_loader: - kv  10:                        phi3.context_length u32              = 131072\n",
      "llama_model_loader: - kv  11:  phi3.rope.scaling.original_context_length u32              = 4096\n",
      "llama_model_loader: - kv  12:                      phi3.embedding_length u32              = 3072\n",
      "llama_model_loader: - kv  13:                   phi3.feed_forward_length u32              = 8192\n",
      "llama_model_loader: - kv  14:                           phi3.block_count u32              = 32\n",
      "llama_model_loader: - kv  15:                  phi3.attention.head_count u32              = 32\n",
      "llama_model_loader: - kv  16:               phi3.attention.head_count_kv u32              = 32\n",
      "llama_model_loader: - kv  17:      phi3.attention.layer_norm_rms_epsilon f32              = 0.000010\n",
      "llama_model_loader: - kv  18:                  phi3.rope.dimension_count u32              = 96\n",
      "llama_model_loader: - kv  19:                        phi3.rope.freq_base f32              = 10000.000000\n",
      "llama_model_loader: - kv  20:                          general.file_type u32              = 7\n",
      "llama_model_loader: - kv  21:              phi3.attention.sliding_window u32              = 262144\n",
      "llama_model_loader: - kv  22:              phi3.rope.scaling.attn_factor f32              = 1.190238\n",
      "llama_model_loader: - kv  23:                       tokenizer.ggml.model str              = llama\n",
      "llama_model_loader: - kv  24:                         tokenizer.ggml.pre str              = default\n",
      "llama_model_loader: - kv  25:                      tokenizer.ggml.tokens arr[str,32064]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n",
      "llama_model_loader: - kv  26:                      tokenizer.ggml.scores arr[f32,32064]   = [-1000.000000, -1000.000000, -1000.00...\n",
      "llama_model_loader: - kv  27:                  tokenizer.ggml.token_type arr[i32,32064]   = [3, 3, 4, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n",
      "llama_model_loader: - kv  28:                tokenizer.ggml.bos_token_id u32              = 1\n",
      "llama_model_loader: - kv  29:                tokenizer.ggml.eos_token_id u32              = 32000\n",
      "llama_model_loader: - kv  30:            tokenizer.ggml.unknown_token_id u32              = 0\n",
      "llama_model_loader: - kv  31:            tokenizer.ggml.padding_token_id u32              = 32000\n",
      "llama_model_loader: - kv  32:               tokenizer.ggml.add_bos_token bool             = false\n",
      "llama_model_loader: - kv  33:               tokenizer.ggml.add_eos_token bool             = false\n",
      "llama_model_loader: - kv  34:                    tokenizer.chat_template str              = {% for message in messages %}{% if me...\n",
      "llama_model_loader: - kv  35:               general.quantization_version u32              = 2\n",
      "llama_model_loader: - type  f32:   67 tensors\n",
      "llama_model_loader: - type q8_0:  130 tensors\n",
      "llm_load_vocab: special tokens cache size = 14\n",
      "llm_load_vocab: token to piece cache size = 0.1685 MB\n",
      "llm_load_print_meta: format           = GGUF V3 (latest)\n",
      "llm_load_print_meta: arch             = phi3\n",
      "llm_load_print_meta: vocab type       = SPM\n",
      "llm_load_print_meta: n_vocab          = 32064\n",
      "llm_load_print_meta: n_merges         = 0\n",
      "llm_load_print_meta: vocab_only       = 0\n",
      "llm_load_print_meta: n_ctx_train      = 131072\n",
      "llm_load_print_meta: n_embd           = 3072\n",
      "llm_load_print_meta: n_layer          = 32\n",
      "llm_load_print_meta: n_head           = 32\n",
      "llm_load_print_meta: n_head_kv        = 32\n",
      "llm_load_print_meta: n_rot            = 96\n",
      "llm_load_print_meta: n_swa            = 262144\n",
      "llm_load_print_meta: n_embd_head_k    = 96\n",
      "llm_load_print_meta: n_embd_head_v    = 96\n",
      "llm_load_print_meta: n_gqa            = 1\n",
      "llm_load_print_meta: n_embd_k_gqa     = 3072\n",
      "llm_load_print_meta: n_embd_v_gqa     = 3072\n",
      "llm_load_print_meta: f_norm_eps       = 0.0e+00\n",
      "llm_load_print_meta: f_norm_rms_eps   = 1.0e-05\n",
      "llm_load_print_meta: f_clamp_kqv      = 0.0e+00\n",
      "llm_load_print_meta: f_max_alibi_bias = 0.0e+00\n",
      "llm_load_print_meta: f_logit_scale    = 0.0e+00\n",
      "llm_load_print_meta: n_ff             = 8192\n",
      "llm_load_print_meta: n_expert         = 0\n",
      "llm_load_print_meta: n_expert_used    = 0\n",
      "llm_load_print_meta: causal attn      = 1\n",
      "llm_load_print_meta: pooling type     = 0\n",
      "llm_load_print_meta: rope type        = 2\n",
      "llm_load_print_meta: rope scaling     = linear\n",
      "llm_load_print_meta: freq_base_train  = 10000.0\n",
      "llm_load_print_meta: freq_scale_train = 1\n",
      "llm_load_print_meta: n_ctx_orig_yarn  = 4096\n",
      "llm_load_print_meta: rope_finetuned   = unknown\n",
      "llm_load_print_meta: ssm_d_conv       = 0\n",
      "llm_load_print_meta: ssm_d_inner      = 0\n",
      "llm_load_print_meta: ssm_d_state      = 0\n",
      "llm_load_print_meta: ssm_dt_rank      = 0\n",
      "llm_load_print_meta: ssm_dt_b_c_rms   = 0\n",
      "llm_load_print_meta: model type       = 3B\n",
      "llm_load_print_meta: model ftype      = Q8_0\n",
      "llm_load_print_meta: model params     = 3.82 B\n",
      "llm_load_print_meta: model size       = 3.78 GiB (8.50 BPW) \n",
      "llm_load_print_meta: general.name     = Phi 3.5 Mini Instruct\n",
      "llm_load_print_meta: BOS token        = 1 '<s>'\n",
      "llm_load_print_meta: EOS token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: UNK token        = 0 '<unk>'\n",
      "llm_load_print_meta: PAD token        = 32000 '<|endoftext|>'\n",
      "llm_load_print_meta: LF token         = 13 '<0x0A>'\n",
      "llm_load_print_meta: EOT token        = 32007 '<|end|>'\n",
      "llm_load_print_meta: max token length = 48\n",
      "llm_load_tensors: ggml ctx size =    0.10 MiB\n",
      "llm_load_tensors:        CPU buffer size =  3872.38 MiB\n",
      ".....................................................................................\n",
      "llama_new_context_with_model: n_ctx      = 4096\n",
      "llama_new_context_with_model: n_batch    = 512\n",
      "llama_new_context_with_model: n_ubatch   = 512\n",
      "llama_new_context_with_model: flash_attn = 0\n",
      "llama_new_context_with_model: freq_base  = 10000.0\n",
      "llama_new_context_with_model: freq_scale = 1\n",
      "llama_kv_cache_init:        CPU KV buffer size =  1536.00 MiB\n",
      "llama_new_context_with_model: KV self size  = 1536.00 MiB, K (f16):  768.00 MiB, V (f16):  768.00 MiB\n",
      "llama_new_context_with_model:        CPU  output buffer size =     0.12 MiB\n",
      "llama_new_context_with_model:        CPU compute buffer size =   300.01 MiB\n",
      "llama_new_context_with_model: graph nodes  = 1286\n",
      "llama_new_context_with_model: graph splits = 1\n",
      "AVX = 1 | AVX_VNNI = 0 | AVX2 = 1 | AVX512 = 1 | AVX512_VBMI = 1 | AVX512_VNNI = 1 | AVX512_BF16 = 0 | FMA = 1 | NEON = 0 | SVE = 0 | ARM_FMA = 0 | F16C = 1 | FP16_VA = 0 | WASM_SIMD = 0 | BLAS = 0 | SSE3 = 1 | SSSE3 = 1 | VSX = 0 | MATMUL_INT8 = 0 | LLAMAFILE = 1 | \n",
      "Model metadata: {'general.quantization_version': '2', 'tokenizer.chat_template': \"{% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'user' %}{{'<|user|>\\n' + message['content'] + '<|end|>\\n'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\\n' + message['content'] + '<|end|>\\n'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\\n' }}{% else %}{{ eos_token }}{% endif %}\", 'phi3.rope.scaling.original_context_length': '4096', 'general.architecture': 'phi3', 'phi3.rope.scaling.attn_factor': '1.190238', 'general.license': 'mit', 'phi3.context_length': '131072', 'general.type': 'model', 'general.license.link': 'https://huggingface.co/microsoft/Phi-3.5-mini-instruct/resolve/main/LICENSE', 'tokenizer.ggml.pre': 'default', 'general.basename': 'Phi-3.5', 'tokenizer.ggml.padding_token_id': '32000', 'phi3.attention.head_count': '32', 'phi3.attention.head_count_kv': '32', 'phi3.attention.layer_norm_rms_epsilon': '0.000010', 'phi3.embedding_length': '3072', 'phi3.rope.dimension_count': '96', 'general.finetune': 'instruct', 'general.file_type': '7', 'phi3.rope.freq_base': '10000.000000', 'phi3.attention.sliding_window': '262144', 'phi3.block_count': '32', 'tokenizer.ggml.model': 'llama', 'phi3.feed_forward_length': '8192', 'general.name': 'Phi 3.5 Mini Instruct', 'tokenizer.ggml.bos_token_id': '1', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '32000', 'general.size_label': 'mini', 'tokenizer.ggml.add_bos_token': 'false', 'tokenizer.ggml.add_eos_token': 'false'}\n",
      "Available chat formats from metadata: chat_template.default\n",
      "Using gguf chat template: {% for message in messages %}{% if message['role'] == 'system' and message['content'] %}{{'<|system|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'user' %}{{'<|user|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% elif message['role'] == 'assistant' %}{{'<|assistant|>\n",
      "' + message['content'] + '<|end|>\n",
      "'}}{% endif %}{% endfor %}{% if add_generation_prompt %}{{ '<|assistant|>\n",
      "' }}{% else %}{{ eos_token }}{% endif %}\n",
      "Using chat eos_token: <|endoftext|>\n",
      "Using chat bos_token: <s>\n"
     ]
    }
   ],
   "source": [
    "model_id = 'microsoft/Phi-3.5-mini-instruct'\n",
    "repo_gguf_id = 'QuantFactory/Phi-3.5-mini-instruct-GGUF'\n",
    "filename_gguf = 'Phi-3.5-mini-instruct.Q8_0.gguf'\n",
    "\n",
    "LOAD_GGUF_CONFIG = {\n",
    "    'n_gpu_layers': 0, # not GPUs\n",
    "    'n_threads': 32, # for CPUs\n",
    "    'n_ctx': 4096  # length context ...\n",
    "}\n",
    "\n",
    "model = SalesAgent(model_id,\n",
    "                   repo_gguf_id,\n",
    "                   filename_gguf,\n",
    "                   LOAD_GGUF_CONFIG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Конвейер Text2Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'dataset/cleaned_coffee.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m cleaning_t2pandas \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m answer: re\u001b[38;5;241m.\u001b[39msub(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpython\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m, answer\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m```\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m1\u001b[39m])\u001b[38;5;241m.\u001b[39mstrip()\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m### загрузка датасета\u001b[39;00m\n\u001b[0;32m---> 12\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mPATH_TO_DATASET\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     13\u001b[0m dataset\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[0;32m~/llm/venv_llm/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[1;32m   1014\u001b[0m     dialect,\n\u001b[1;32m   1015\u001b[0m     delimiter,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[1;32m   1023\u001b[0m )\n\u001b[1;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[0;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm/venv_llm/lib/python3.12/site-packages/pandas/io/parsers/readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[0;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m \u001b[43mTextFileReader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[1;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[0;32m~/llm/venv_llm/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/llm/venv_llm/lib/python3.12/site-packages/pandas/io/parsers/readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[1;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m \u001b[43mget_handle\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1883\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcompression\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcompression\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1885\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmemory_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmemory_map\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1886\u001b[0m \u001b[43m    \u001b[49m\u001b[43mis_text\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_text\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1887\u001b[0m \u001b[43m    \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mencoding_errors\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstrict\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptions\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mstorage_options\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1889\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[0;32m~/llm/venv_llm/lib/python3.12/site-packages/pandas/io/common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[1;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[1;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[1;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[0;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\n\u001b[1;32m    874\u001b[0m \u001b[43m            \u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    875\u001b[0m \u001b[43m            \u001b[49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    876\u001b[0m \u001b[43m            \u001b[49m\u001b[43mencoding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mioargs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    877\u001b[0m \u001b[43m            \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    878\u001b[0m \u001b[43m            \u001b[49m\u001b[43mnewline\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    879\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[1;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'dataset/cleaned_coffee.csv'"
     ]
    }
   ],
   "source": [
    "USER, ASSISTANT = [], [] # для сохранения контекста диалога\n",
    "PATH_TO_DATASET = 'dataset/cleaned_coffee.csv'\n",
    "\n",
    "delimeter = '\\n\\n' + ('='* 100) + '\\n\\n'\n",
    "\n",
    "# Interpreter Python\n",
    "REPL = PythonREPL()\n",
    "\n",
    "cleaning_t2pandas = lambda answer: re.sub('python', '', answer.split('```')[1]).strip()\n",
    "\n",
    "### загрузка датасета\n",
    "dataset = pd.read_csv(PATH_TO_DATASET)\n",
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Llama.generate: 1032 prefix-match hit, remaining 1 prompt tokens to eval\n"
     ]
    }
   ],
   "source": [
    "### Инициализация диалога\n",
    "messages_init = [\n",
    "    {'role': 'system', 'content': SYS_PROMPT_INIT_DIALOGUE(CONTEXT_NAPOLEON_IT, SALES_SCRIPT)},\n",
    "    {'role': 'user', 'content': USER_INIT_PROMPT}\n",
    "]\n",
    "\n",
    "started_message_to_customer = model.inference_agent(messages_init)\n",
    "ASSISTANT.append(started_message_to_customer) # запоминаем диалог со стороны ассистента\n",
    "\n",
    "print(f'STEP #1:\\n\\nСООБЩЕНИЕ МОДЕЛИ ДЛЯ ИНИЦИАЛИЗАЦИИ: {started_message_to_customer}', end=delimeter)\n",
    "\n",
    "\n",
    "### Продолжение диалога в формате ожидания ответа от клиента\n",
    "while True:\n",
    "\n",
    "    # приходит ответ от клиента\n",
    "    customer = input()\n",
    "    USER.append(customer) # запоминаем диалог со стороны клиента\n",
    "\n",
    "    print(f'STEP #2:\\n\\nОТВЕТ КЛИЕНТА: {customer}', end=delimeter)\n",
    "\n",
    "    # проверяем intent\n",
    "    messages_intent = [\n",
    "        {'role': 'system', 'content': SYS_PROMPT_FOR_INTENTION},\n",
    "        {'role': 'user', 'content': customer}\n",
    "    ]\n",
    "    label_intention = model.inference_agent(messages_intent)\n",
    "\n",
    "    print(f'ВЫЯВЛЯЕМ НАМЕРЕНИЕ:\\n\\nНАМЕРЕНИЕ: {label_intention}', end=delimeter)\n",
    "\n",
    "    # без RAG'a\n",
    "    if 'да' in label_intention:\n",
    "\n",
    "        print('КОНТЕКСТ:\\n{CONTEXT_NAPOLEON_IT}', end=delimeter)\n",
    "\n",
    "        messages = model.formating_chat_template(system=SYS_PROMPT_CONTINUE_DIALOGUE,\n",
    "                                                 context=CONTEXT_NAPOLEON_IT,\n",
    "                                                 user=USER,\n",
    "                                                 assistant=ASSISTANT)\n",
    "        agent = model.inference_agent(messages)\n",
    "\n",
    "    # с RAG'ом\n",
    "    elif 'нет' in label_intention:\n",
    "\n",
    "        # RAG\n",
    "        path_to_data, columns, example = PATH_TO_DATASET, dataset.columns.to_list(), dataset.sample(1)\n",
    "\n",
    "        messages = model.formating_chat_template(system=SYS_PROMPT_TEXT_2_PANDAS,\n",
    "                                                 context=[path_to_data, columns, example],\n",
    "                                                 user=USER,\n",
    "                                                 assistant=ASSISTANT)\n",
    "        rag_agent = model.inference_agent(messages)\n",
    "\n",
    "        print(f'TEXT_2_PANDAS ЗАПРОС МОДЕЛИ:\\n{rag_agent}', end=delimeter)\n",
    "\n",
    "        CONTEXT_RAG = REPL.run(rag_agent)\n",
    "        CONTEXT_RAG = cleaning_t2pandas(CONTEXT_RAG)\n",
    "\n",
    "        print(f'ПОЛУЧЕННЫЙ КОНТЕКСТ С RAGA:\\n{CONTEXT_RAG}', end=delimeter)\n",
    "        \n",
    "        messages = model.formating_chat_template(system=SYS_PROMPT_CONTINUE_DIALOGUE,\n",
    "                                                 context=[CONTEXT_RAG],\n",
    "                                                 user=customer,\n",
    "                                                 assistant=ASSISTANT)\n",
    "        agent = model.inference_agent(messages)\n",
    "\n",
    "        print(f'ОТВЕТ МОДЕЛИ ПО RAGy:\\n{agent}', end=delimeter)\n",
    "\n",
    "\n",
    "    ASSISTANT.append(agent) # запоминаем диалог со стороны ассистента"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
