{"cells":[{"cell_type":"markdown","metadata":{},"source":["#### Classic RAG"]},{"cell_type":"code","execution_count":1,"metadata":{"execution":{"iopub.execute_input":"2024-09-07T12:49:45.749992Z","iopub.status.busy":"2024-09-07T12:49:45.749614Z","iopub.status.idle":"2024-09-07T12:51:09.508794Z","shell.execute_reply":"2024-09-07T12:51:09.507465Z","shell.execute_reply.started":"2024-09-07T12:49:45.749950Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["--2024-09-07 12:49:46--  https://hf.rst.im/IlyaGusev/saiga_llama3_8b_gguf/resolve/main/model-q4_K.gguf\n","Resolving hf.rst.im (hf.rst.im)... 104.21.14.234, 172.67.160.195, 2606:4700:3031::6815:eea, ...\n","Connecting to hf.rst.im (hf.rst.im)|104.21.14.234|:443... connected.\n","HTTP request sent, awaiting response... 302 Found\n","Location: https://p.rst.im/q/cdn-lfs-us-1.huggingface.co/repos/72/2a/722a5c219c9453cf664acb11f902364b7110775d7abb8e43609fe33a44d87daf/24286f1721cd6b594530a7a536cb94d5d10568b86eec96f7139bb62cf39ea58b?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1725972587&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNTk3MjU4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzcyLzJhLzcyMmE1YzIxOWM5NDUzY2Y2NjRhY2IxMWY5MDIzNjRiNzExMDc3NWQ3YWJiOGU0MzYwOWZlMzNhNDRkODdkYWYvMjQyODZmMTcyMWNkNmI1OTQ1MzBhN2E1MzZjYjk0ZDVkMTA1NjhiODZlZWM5NmY3MTM5YmI2MmNmMzllYTU4Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=d5Irfrlj7gLEAqkagSIH%7Ef3jgp7P%7Emwn7fIVVcqb-C1wpwzAwXtMnVF4rJyOMBBOz1Gr9h0Mfjf3vF4ZKsnLqfdNI9EECxVI3PmWxxUwtSzM0%7E4LbTFaTHzGm6fzLSbz1qQ4IrlFWz9r3bC8ZWokJ%7ENmw0q7HQwdVrFo%7EWVtC8vu0XyDu4cYunA-HK%7EPBRNi0wh315Whl1OGxTD1oNbtWJoPUJg2JquQ7sAIaLdXx2lSZXhuxri2Hb8cIYH3ofhtjyCT-vJIHQndlO8-rAEWUSz8mi7vV8wV1aszc9ieeUeIeA707R1dDO%7EvYLnT1L%7EEyUFUrhr4TWFGPql3oXrW4w__&Key-Pair-Id=K24J24Z295AEI9 [following]\n","--2024-09-07 12:50:52--  https://p.rst.im/q/cdn-lfs-us-1.huggingface.co/repos/72/2a/722a5c219c9453cf664acb11f902364b7110775d7abb8e43609fe33a44d87daf/24286f1721cd6b594530a7a536cb94d5d10568b86eec96f7139bb62cf39ea58b?response-content-disposition=inline%3B+filename*%3DUTF-8''model-q4_K.gguf%3B+filename%3D%22model-q4_K.gguf%22%3B&Expires=1725972587&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcyNTk3MjU4N319LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzcyLzJhLzcyMmE1YzIxOWM5NDUzY2Y2NjRhY2IxMWY5MDIzNjRiNzExMDc3NWQ3YWJiOGU0MzYwOWZlMzNhNDRkODdkYWYvMjQyODZmMTcyMWNkNmI1OTQ1MzBhN2E1MzZjYjk0ZDVkMTA1NjhiODZlZWM5NmY3MTM5YmI2MmNmMzllYTU4Yj9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=d5Irfrlj7gLEAqkagSIH~f3jgp7P~mwn7fIVVcqb-C1wpwzAwXtMnVF4rJyOMBBOz1Gr9h0Mfjf3vF4ZKsnLqfdNI9EECxVI3PmWxxUwtSzM0~4LbTFaTHzGm6fzLSbz1qQ4IrlFWz9r3bC8ZWokJ~Nmw0q7HQwdVrFo~WVtC8vu0XyDu4cYunA-HK~PBRNi0wh315Whl1OGxTD1oNbtWJoPUJg2JquQ7sAIaLdXx2lSZXhuxri2Hb8cIYH3ofhtjyCT-vJIHQndlO8-rAEWUSz8mi7vV8wV1aszc9ieeUeIeA707R1dDO~vYLnT1L~EyUFUrhr4TWFGPql3oXrW4w__&Key-Pair-Id=K24J24Z295AEI9\n","Resolving p.rst.im (p.rst.im)... 172.67.160.195, 104.21.14.234, 2606:4700:3031::6815:eea, ...\n","Connecting to p.rst.im (p.rst.im)|172.67.160.195|:443... connected.\n","HTTP request sent, awaiting response... 200 OK\n","Length: 4920734016 (4.6G) [binary/octet-stream]\n","Saving to: 'model-q4_K.gguf'\n","\n","model-q4_K.gguf       1%[                    ]  77.12M  5.06MB/s    eta 15m 8s ^C\n"]}],"source":["!wget https://hf.rst.im/IlyaGusev/saiga_llama3_8b_gguf/resolve/main/model-q4_K.gguf"]},{"cell_type":"code","execution_count":2,"metadata":{"execution":{"iopub.execute_input":"2024-09-07T12:51:09.511485Z","iopub.status.busy":"2024-09-07T12:51:09.511069Z","iopub.status.idle":"2024-09-07T12:51:11.014804Z","shell.execute_reply":"2024-09-07T12:51:11.013652Z","shell.execute_reply.started":"2024-09-07T12:51:09.511434Z"},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["^C\n","Traceback (most recent call last):\n","  File \"/opt/conda/bin/pip\", line 6, in <module>\n","    from pip._internal.cli.main import main\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/main.py\", line 10, in <module>\n","    from pip._internal.cli.autocompletion import autocomplete\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/autocompletion.py\", line 10, in <module>\n","    from pip._internal.cli.main_parser import create_main_parser\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/main_parser.py\", line 9, in <module>\n","    from pip._internal.build_env import get_runnable_pip\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/build_env.py\", line 19, in <module>\n","    from pip._internal.cli.spinners import open_spinner\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/cli/spinners.py\", line 9, in <module>\n","    from pip._internal.utils.logging import get_indentation\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_internal/utils/logging.py\", line 13, in <module>\n","    from pip._vendor.rich.console import (\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/rich/console.py\", line 51, in <module>\n","    from ._log_render import FormatTimeCallable, LogRender\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/rich/_log_render.py\", line 5, in <module>\n","    from .text import Text, TextType\n","  File \"/opt/conda/lib/python3.10/site-packages/pip/_vendor/rich/text.py\", line 25, in <module>\n","    from .emoji import EmojiVariant\n","  File \"<frozen importlib._bootstrap>\", line 1027, in _find_and_load\n","  File \"<frozen importlib._bootstrap>\", line 1006, in _find_and_load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 674, in _load_unlocked\n","  File \"<frozen importlib._bootstrap>\", line 577, in module_from_spec\n","  File \"<frozen importlib._bootstrap>\", line 556, in _init_module_attrs\n","  File \"<frozen importlib._bootstrap>\", line 397, in cached\n","  File \"<frozen importlib._bootstrap_external>\", line 513, in _get_cached\n","  File \"<frozen importlib._bootstrap_external>\", line 406, in cache_from_source\n","  File \"<frozen importlib._bootstrap_external>\", line 134, in _path_split\n","  File \"<frozen importlib._bootstrap_external>\", line 134, in <genexpr>\n","KeyboardInterrupt\n"]}],"source":["!pip install bitsandbytes"]},{"cell_type":"code","execution_count":2,"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","execution":{"iopub.execute_input":"2024-09-07T10:33:52.723386Z","iopub.status.busy":"2024-09-07T10:33:52.722907Z","iopub.status.idle":"2024-09-07T10:33:52.729866Z","shell.execute_reply":"2024-09-07T10:33:52.728926Z","shell.execute_reply.started":"2024-09-07T10:33:52.723334Z"},"trusted":true},"outputs":[],"source":["import pandas as pd\n","import torch\n","import transformers\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.metrics.pairwise import cosine_similarity\n","from transformers import AutoModelForCausalLM, AutoTokenizer, GenerationConfig\n","from transformers import AutoTokenizer, AutoModel\n"]},{"cell_type":"code","execution_count":13,"metadata":{"execution":{"iopub.execute_input":"2024-09-07T10:33:52.732015Z","iopub.status.busy":"2024-09-07T10:33:52.731592Z","iopub.status.idle":"2024-09-07T10:33:52.760075Z","shell.execute_reply":"2024-09-07T10:33:52.758991Z","shell.execute_reply.started":"2024-09-07T10:33:52.731967Z"},"trusted":true},"outputs":[],"source":["class BertEmbedder:\n","    def __init__(self, model_name, device='cpu'):\n","        \"\"\"\n","        Инициализация BERT модели и токенайзера.\n","        \n","        :param model_name: Название предобученной модели).\n","        :param device: Устройство для вычислений ('cuda' для GPU или 'cpu' для CPU). Если None, автоматически выбирается доступное устройство.\n","        \"\"\"\n","        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') if device is None else device\n","        print(device)\n","        \n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.model = AutoModel.from_pretrained(model_name)\n","        if self.device == 'cuda':\n","            self.model.cuda()\n","\n","    def get_embedding(self, text):\n","        \"\"\"\n","        Преобразует текст в эмбеддинг с использованием BERT.\n","        \n","        :param text: Входной текст для преобразования.\n","        :return: Эмбеддинг текста в виде numpy массива.\n","        \"\"\"\n","        tokenized_text = self.tokenizer(text, padding=True, truncation=True, return_tensors='pt')\n","        \n","        with torch.no_grad():\n","            model_output = self.model(**{k: v.to(self.device) for k, v in tokenized_text.items()})\n","        \n","        embeddings = model_output.last_hidden_state[:, 0, :]\n","        embeddings = torch.nn.functional.normalize(embeddings)\n","        return embeddings[0].cpu().numpy()\n","    \n","    def retrieve_reviews(self, query, reviews_df, top_n=3):\n","        review_texts = reviews_df['Review Text'].fillna('').tolist()\n","        \n","        reviews_embeddings = self.get_embedding(review_texts)\n","        query_embedding = self.get_embedding(query)\n","\n","        similarities = cosine_similarity(query_embedding, reviews_embeddings).flatten()\n","        top_indices = similarities.argsort()[-top_n:][::-1]\n","\n","        return reviews_df.iloc[top_indices]\n","    \n","class LLMModel:\n","    def __init__(self, model_name, device='cpu'):\n","        \"\"\"\n","        Инициализация LLM модели.\n","        \n","        :param model_name: Название предобученной модели.\n","        :param device: Устройство для вычислений ('cuda' для GPU или 'cpu' для CPU). Если None, автоматически выбирается доступное устройство.\n","        \"\"\"\n","        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n","        self.generation_config = GenerationConfig.from_pretrained(model_name)\n","        self.model = transformers.AutoModelForCausalLM.from_pretrained(\n","            model_name,\n","            load_in_8bit=True,\n","            torch_dtype=torch.bfloat16,\n","            device_map=\"auto\"\n","        )\n","        self.model.eval()\n","        \n","    def generate_first_message(self):\n","        context_1 = \"\"\"\n","        \"\"\"\n","        system_prompt = f\"\"\"Ты профессиональный агент-продавец, опыт которого в продажах больше 10 лет.\n","            Ты инициализируешь диалог с клиентом и твоя цель - заинтересовать его продуктом, который ты предлагаешь.\n","            Ты должен быть достаточно убедительным и дружелюбным.\n","            Твои ответы должны быть только на Русском.\n","\n","            Продукт, который ты предлагаешь описан тут:\n","            {context_1}\n","\n","            Покажи по-настоящему свои навыки продавца и заинтересуй клиента:\n","            \"\"\"\n","\n","        user_prompt = f\"Я - потенциальный клиент. Инициируй со мной диалог: Напиши мне письмо, где рассказываешь о продукте Napoleon Отзывы\"\n","\n","        prompt = self.tokenizer.apply_chat_template([\n","            {\"role\": \"system\", \"content\": system_prompt},\n","            {\"role\": \"user\", \"content\": user_prompt},\n","        ], tokenize=False, add_generation_prompt=True)\n","\n","        data = self.tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n","        data = {k: v.to(self.model.device) for k, v in data.items()}\n","        output_ids = self.model.generate(**data, generation_config=self.generation_config)[0]\n","        output_ids = output_ids[len(data[\"input_ids\"][0]):]\n","        output = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n","        print(output)\n","\n","    def generate_response(self, client_query, relevant_reviews):\n","        review_texts = '\\n'.join(relevant_reviews['Review Text'].tolist())\n","        system_prompt = f\"\"\"Ты профессиональный агент-продавец, опыт которого в продажах больше 10 лет.\n","            Ты ведешь диалог с клиентом, которому предлагаешь сотрудничество по продукту \"Napoleon IT Отзывы\".\n","            Ты должен обрабатывать любые вопросы потенциального клиента, предоставляя только релевантную и достовернную информацию, используя контекст.\n","            Не груби и будь дружелюбным собесебником.\n","            Отвечай только на Русском.\n","\n","            Обработай запрос клиента и выдай точную информацию:\n","            Вот отзывы о товарах клиента: {review_texts}. Ответь на вопрос клиента на опираясь на них.\n","            \"\"\"\n","        user_prompt = f\"Клиент спросил: '{client_query}'\"\n","\n","        prompt = self.tokenizer.apply_chat_template([\n","            {\"role\": \"system\", \"content\": system_prompt},\n","            {\"role\": \"user\", \"content\": user_prompt},\n","        ], tokenize=False, add_generation_prompt=True)\n","\n","        data = self.tokenizer(prompt, return_tensors=\"pt\", add_special_tokens=False)\n","        data = {k: v.to(self.model.device) for k, v in data.items()}\n","        output_ids = self.model.generate(**data, generation_config=self.generation_config)[0]\n","        output_ids = output_ids[len(data[\"input_ids\"][0]):]\n","        output = self.tokenizer.decode(output_ids, skip_special_tokens=True).strip()\n","        print(output)"]},{"cell_type":"code","execution_count":14,"metadata":{"execution":{"iopub.execute_input":"2024-09-07T10:33:52.761719Z","iopub.status.busy":"2024-09-07T10:33:52.761373Z","iopub.status.idle":"2024-09-07T10:33:52.775830Z","shell.execute_reply":"2024-09-07T10:33:52.774977Z","shell.execute_reply.started":"2024-09-07T10:33:52.761685Z"},"trusted":true},"outputs":[],"source":["class SalesBotHandler:\n","    def __init__(self,\n","                 bert_model_name='cointegrated/rubert-tiny', \n","                 llm_model_name='IlyaGusev/saiga_llama3_8b',\n","                 reviews_path = \"/kaggle/input/aiproducthack-eldinero/cleaned_coffee.csv\"):\n","        self.bert_embedder = BertEmbedder(model_name=bert_model_name)\n","        self.llm_model = LLMModel(model_name=llm_model_name)\n","        self.reviews = self.load_reviews(reviews_path)\n","    \n","    def load_reviews(self, reviews_path):\n","        return pd.read_csv(reviews_path)\n","    \n","    def process_customer_input(self, customer_input):\n","        \"\"\"\n","        Метод для ведения диалога с клиентом с использованием LLM Model.\n","        \"\"\"\n","        relevant_reviews = self.bert_embedder.retrieve_reviews(customer_input, self.reviews)\n","        return self.llm_model.generate_response(customer_input, relevant_reviews)\n","\n","    def generate_first_message(self):\n","        \"\"\"\n","        Метод для генерации первого письма с использованием LLM Model.\n","        \"\"\"\n","        return self.llm_model.generate_first_message()"]},{"cell_type":"code","execution_count":15,"metadata":{"collapsed":true,"execution":{"iopub.execute_input":"2024-09-07T10:33:52.778800Z","iopub.status.busy":"2024-09-07T10:33:52.778402Z","iopub.status.idle":"2024-09-07T10:35:53.462447Z","shell.execute_reply":"2024-09-07T10:35:53.461475Z","shell.execute_reply.started":"2024-09-07T10:33:52.778755Z"},"jupyter":{"outputs_hidden":true},"trusted":true},"outputs":[{"name":"stdout","output_type":"stream","text":["cuda\n"]},{"name":"stderr","output_type":"stream","text":["/opt/conda/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n","  warnings.warn(\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ec2b2440bced4fd68264d7ce5d6d61e8","version_major":2,"version_minor":0},"text/plain":["tokenizer_config.json:   0%|          | 0.00/51.0k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2b1023b42a6e426d8249a7fe2ac220c9","version_major":2,"version_minor":0},"text/plain":["tokenizer.json:   0%|          | 0.00/9.09M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"756d49a3bfd64a5984ed4534c408bac7","version_major":2,"version_minor":0},"text/plain":["special_tokens_map.json:   0%|          | 0.00/446 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"b6247b830418487987344d700cf0c289","version_major":2,"version_minor":0},"text/plain":["generation_config.json:   0%|          | 0.00/277 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0c4d1c84eb634aca8b67b975623750c6","version_major":2,"version_minor":0},"text/plain":["config.json:   0%|          | 0.00/689 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ecd745ad88c8490d9dd65b86f89fbf59","version_major":2,"version_minor":0},"text/plain":["model.safetensors.index.json:   0%|          | 0.00/23.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"c91b4cfea0a54a3c86e4ae9856927fae","version_major":2,"version_minor":0},"text/plain":["Downloading shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"ff24097e93f24942bf4c8579a27243c1","version_major":2,"version_minor":0},"text/plain":["model-00001-of-00004.safetensors:   0%|          | 0.00/4.98G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d7cf6c2ec9764c68a4940f32b40d25fb","version_major":2,"version_minor":0},"text/plain":["model-00002-of-00004.safetensors:   0%|          | 0.00/5.00G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"a8e6401da873408a9db31a43039ffea2","version_major":2,"version_minor":0},"text/plain":["model-00003-of-00004.safetensors:   0%|          | 0.00/4.92G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"8e5a439d64db44c7942d750866643493","version_major":2,"version_minor":0},"text/plain":["model-00004-of-00004.safetensors:   0%|          | 0.00/1.17G [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"246a01dbb30f4216b5805c69c3d157b5","version_major":2,"version_minor":0},"text/plain":["Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"}],"source":["handler = SalesBotHandler()"]},{"cell_type":"code","execution_count":16,"metadata":{"execution":{"iopub.execute_input":"2024-09-07T10:35:53.464947Z","iopub.status.busy":"2024-09-07T10:35:53.464099Z","iopub.status.idle":"2024-09-07T10:35:57.049997Z","shell.execute_reply":"2024-09-07T10:35:57.048414Z","shell.execute_reply.started":"2024-09-07T10:35:53.464876Z"},"trusted":true},"outputs":[{"ename":"OutOfMemoryError","evalue":"CUDA out of memory. Tried to allocate 1022.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 227.12 MiB is free. Process 3141 has 15.66 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# handler.generate_first_message()\u001b[39;00m\n\u001b[1;32m      3\u001b[0m client_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mКак клиенты оценивают качество товара?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m----> 4\u001b[0m relevant_reviews \u001b[38;5;241m=\u001b[39m \u001b[43mhandler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_customer_input\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclient_query\u001b[49m\u001b[43m)\u001b[49m\n","Cell \u001b[0;32mIn[14], line 17\u001b[0m, in \u001b[0;36mSalesBotHandler.process_customer_input\u001b[0;34m(self, customer_input)\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprocess_customer_input\u001b[39m(\u001b[38;5;28mself\u001b[39m, customer_input):\n\u001b[1;32m     14\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;124;03m    Метод для ведения диалога с клиентом с использованием LLM Model.\u001b[39;00m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m---> 17\u001b[0m     relevant_reviews \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbert_embedder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve_reviews\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcustomer_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreviews\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mllm_model\u001b[38;5;241m.\u001b[39mgenerate_response(customer_input, relevant_reviews)\n","Cell \u001b[0;32mIn[13], line 36\u001b[0m, in \u001b[0;36mBertEmbedder.retrieve_reviews\u001b[0;34m(self, query, reviews_df, top_n)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mretrieve_reviews\u001b[39m(\u001b[38;5;28mself\u001b[39m, query, reviews_df, top_n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     34\u001b[0m     review_texts \u001b[38;5;241m=\u001b[39m reviews_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReview Text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mfillna(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mtolist()\n\u001b[0;32m---> 36\u001b[0m     reviews_embeddings \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_embedding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreview_texts\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     query_embedding \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_embedding(query)\n\u001b[1;32m     39\u001b[0m     similarities \u001b[38;5;241m=\u001b[39m cosine_similarity(query_embedding, reviews_embeddings)\u001b[38;5;241m.\u001b[39mflatten()\n","Cell \u001b[0;32mIn[13], line 27\u001b[0m, in \u001b[0;36mBertEmbedder.get_embedding\u001b[0;34m(self, text)\u001b[0m\n\u001b[1;32m     24\u001b[0m tokenized_text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenizer(text, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m---> 27\u001b[0m     model_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m{\u001b[49m\u001b[43mk\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtokenized_text\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitems\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m model_output\u001b[38;5;241m.\u001b[39mlast_hidden_state[:, \u001b[38;5;241m0\u001b[39m, :]\n\u001b[1;32m     30\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mnormalize(embeddings)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:1141\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1134\u001b[0m \u001b[38;5;66;03m# Prepare head mask if needed\u001b[39;00m\n\u001b[1;32m   1135\u001b[0m \u001b[38;5;66;03m# 1.0 in head_mask indicate we keep the head\u001b[39;00m\n\u001b[1;32m   1136\u001b[0m \u001b[38;5;66;03m# attention_probs has shape bsz x n_heads x N x N\u001b[39;00m\n\u001b[1;32m   1137\u001b[0m \u001b[38;5;66;03m# input head_mask has shape [num_heads] or [num_hidden_layers x num_heads]\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m \u001b[38;5;66;03m# and head_mask is converted to shape [num_hidden_layers x batch x num_heads x seq_length x seq_length]\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m head_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_head_mask(head_mask, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m-> 1141\u001b[0m encoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mencoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1142\u001b[0m \u001b[43m    \u001b[49m\u001b[43membedding_output\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1143\u001b[0m \u001b[43m    \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mextended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1144\u001b[0m \u001b[43m    \u001b[49m\u001b[43mhead_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhead_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1145\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1146\u001b[0m \u001b[43m    \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mencoder_extended_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpast_key_values\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpast_key_values\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1148\u001b[0m \u001b[43m    \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1149\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1150\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_hidden_states\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43moutput_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1151\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_dict\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_dict\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1152\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1153\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m encoder_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m   1154\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler(sequence_output) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpooler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:694\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    683\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_gradient_checkpointing_func(\n\u001b[1;32m    684\u001b[0m         layer_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m,\n\u001b[1;32m    685\u001b[0m         hidden_states,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    691\u001b[0m         output_attentions,\n\u001b[1;32m    692\u001b[0m     )\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 694\u001b[0m     layer_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mlayer_module\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    695\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    696\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    697\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlayer_head_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    698\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_hidden_states\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    699\u001b[0m \u001b[43m        \u001b[49m\u001b[43mencoder_attention_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    700\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpast_key_value\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    701\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_attentions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    704\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m layer_outputs[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m use_cache:\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:626\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[0;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[1;32m    623\u001b[0m     cross_attn_present_key_value \u001b[38;5;241m=\u001b[39m cross_attention_outputs[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    624\u001b[0m     present_key_value \u001b[38;5;241m=\u001b[39m present_key_value \u001b[38;5;241m+\u001b[39m cross_attn_present_key_value\n\u001b[0;32m--> 626\u001b[0m layer_output \u001b[38;5;241m=\u001b[39m \u001b[43mapply_chunking_to_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    627\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfeed_forward_chunk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mchunk_size_feed_forward\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mseq_len_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\n\u001b[1;32m    628\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    629\u001b[0m outputs \u001b[38;5;241m=\u001b[39m (layer_output,) \u001b[38;5;241m+\u001b[39m outputs\n\u001b[1;32m    631\u001b[0m \u001b[38;5;66;03m# if decoder, return the attn key/values as the last output\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/pytorch_utils.py:239\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[0;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[1;32m    236\u001b[0m     \u001b[38;5;66;03m# concatenate output at same dimension\u001b[39;00m\n\u001b[1;32m    237\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcat(output_chunks, dim\u001b[38;5;241m=\u001b[39mchunk_dim)\n\u001b[0;32m--> 239\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minput_tensors\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:639\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[0;34m(self, attention_output)\u001b[0m\n\u001b[1;32m    637\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfeed_forward_chunk\u001b[39m(\u001b[38;5;28mself\u001b[39m, attention_output):\n\u001b[1;32m    638\u001b[0m     intermediate_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintermediate(attention_output)\n\u001b[0;32m--> 639\u001b[0m     layer_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m(\u001b[49m\u001b[43mintermediate_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    640\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m layer_output\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/transformers/models/bert/modeling_bert.py:553\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[0;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[1;32m    551\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(hidden_states)\n\u001b[1;32m    552\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m--> 553\u001b[0m hidden_states \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mLayerNorm(\u001b[43mhidden_states\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_tensor\u001b[49m)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m hidden_states\n","\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 1022.00 MiB. GPU 0 has a total capacity of 15.89 GiB of which 227.12 MiB is free. Process 3141 has 15.66 GiB memory in use. Of the allocated memory 14.07 GiB is allocated by PyTorch, and 1.31 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"]}],"source":["# handler.generate_first_message()\n","\n","client_query = \"Как клиенты оценивают качество товара?\"\n","relevant_reviews = handler.process_customer_input(client_query)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":[]}],"metadata":{"kaggle":{"accelerator":"none","dataSources":[{"datasetId":5655567,"sourceId":9334355,"sourceType":"datasetVersion"}],"dockerImageVersionId":30761,"isGpuEnabled":false,"isInternetEnabled":true,"language":"python","sourceType":"notebook"},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.12.3"}},"nbformat":4,"nbformat_minor":4}
